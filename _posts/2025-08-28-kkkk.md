---
title: "kkkk"
date: 2025-08-28 17:30:36 +0900
categories: [기술]
tags: [Python]
toc: true
comments: false
mermaid: true
math: true
---

# LangChain으로 구현하는 RAG 파이프라인: CLIP에서 Ollama까지

RAG는 검색으로 관련 정보를 찾고 그 정보를 바탕으로 LM이 답을 생성하는 방식입니다. LangChain은 이 흐름을 모듈형으로 연결하는 프레임워크입니다. 아래 핵심 요소를 한눈에 정리합니다.

- CLIP: 텍스트와 이미지를 함께 인코딩하는 멀티모달 모델로 문서/이미지 임베딩의 기초가 됩니다.  
- Vectorstore: 임베딩 벡터를 저장하고 유사도 검색을 수행하는 저장소입니다.  
- Document loader: 원문 문서를 수집하고 표준화하는 단계입니다.  
- Embedding model: 텍스트/멀티모달 데이터를 벡터로 변환합니다.  
- RAG with LangChain: 질의 → 검색 → LM 생성의 엔드투엔드 파이프라인입니다.  
- Agent / Tool: 외부 API 호출 등 자동화를 담당하는 실행 유닛입니다.  
- Memory: 대화 맥락과 세션 정보를 유지합니다.  
- Chain / LM: 모듈 간 연결과 생성 모델의 흐름을 관리합니다.  
- Graph: 지식의 연결 구조를 시각화하고 탐색합니다.  
- Ollama / PART: 로컬 LLM 서비스 활용과 구성 패턴입니다.

작동 원리는 짧습니다. 질의가 들어오면 벡터스토어에서 관련 문서를 검색하고, 이를 LM에 보강해 최종 답을 만듭니다. 필요 시 Agent가 Tool로 외부 작업을 수행합니다. 로컬 배포(Ollama)도 가능합니다.

결론적으로 RAG+LangChain은 검색-생성의 결합과 모듈화된 파이프라인 설계의 핵심입니다.